# Python  



# Numpy

#### import 

 ```python
 import numpy as np  
 ```

#### array

```python
A = np.array([[1, 2], [3, 4]])
```

### 연산

```python
A = np.array([[1, 2], [3, 4]])

# 일반 연산
print(A * 3)
print(A + A)
print(A - A)
print(A ** 2)
print(3 ** A)
print(A * A)


# 비교 연산 (같은 차원에서의 연산)
a = np.array([1, 2, 3, 4])
b = np.array([4, 2, 2, 4])

print(a == b)
>> [False, True, False, True]

print(a > b)
>> [False, False, True, False]


# 논리 연산  
a = np.array([1, 1, 0, 0], dtype=bool)
b = np.array([1, 0, 1, 0], dtype=bool)

np.logical_or(a, b)
>> [True, True, True, False]

np.logical_and(a, b)
>> [True, False, False, False]
```

#### **np.dot(x,y)**

```python
# 내적
x = np.array([[1, 2], [3, 4]])
y = np.array([[3, 4], [3, 2]])

print(np.dot(x, y))
```

#### **np.transpose(A)**

```python
A = np.array([[1, 2, 3], [4, 5, 6]])
print(A.T)

>> [[1 4]
   [2 5]
   [3 6]]
```

#### **np.linalg.inv(A)**

```python
A = np.array([[1, 2], [3, 4]])
print(np.linalg.inv(A))
```



#### Reductions

```python
a = np.array([1, 2, 3, 4, 5])

np.sum(a)
>> 15

a.sum()
>> 15

a.min()
>> 1

a.max()
>> 5

a.argmin()
>> 0

a.argmax()
>> 4
```

#### Logical Reductions  

```python
a = np.array([True, True, True])
b = np.array([True, True, False])


# np.all(a) : Array 내의 모든 값이 True인가?
np.all(a)
>> True

np.all(b)
>> False

# np.any(a) : Array 내의 값이 하나라도 True인가?
np.any(a)
>> True

np.any(b)
>> True
```

#### Statistical Reductions  

```python
x = np.array([1, 2, 3, 1])

# np.mean(x) : 평균값
# np.median(x) : 중간값  
# np.std(x) : 표준편차
# np.var(x) : 분산
```



# Pandas  

## 파일 저장, 불러오기

### **pd.read_csv()**

```python
# 지정된 경로의 파일 이름을 가진 파일을 불러옴

"""
pd.read_csv()
	'파일 경로' : 불러올 파일의 경로와 이름 
	encoding : 부호화할 알고리즘 방식
		- 'utf-8' : 한글 인코딩 1
		- 'euc-kr' : 한글 인코딩 2
"""

data = pd.read_csv('/content/data/breast-cancser.csv', encoding='utf-8')
```

## 데이터 확인  

### 전체 데이터  

#### **df.shape()**

```python
# 전체 데이터의 열과 행을 확인할 수 있음
print(data.shape)
>> (683, 11)
```

df.describe()

```python
# 전체 데이터의 통계량을 요약해서 보여줌  
pd.DataFrame(X_scaled_minmax_train).describe()
```



### 범주형 데이터  

#### df.value_counts()

```python
# 해당 레이블의 변수 빈도를 보여준다.  
# 컬럼을 지정해서 사용할 경우 해당 컬럼에서의 레이블 변수 빈도를 보여준다.  

"""
df['Columns_name'].value_counts()
	sort : 값의 정렬 여부
		- False : 정렬 X
		- True : 값을 오름차순으로 정렬해줌
"""

daat['Class'].value_counts(sort=False)
>> 0    444
   1    239
   Name: Class, dtype: int64
```

## 데이터 전처리  

### 전체 데이터

#### **df.loc[]**

```python
# 주어진 행과 열부터, 주어진 행과 열을 포함해서 데이터를 선택한 후 잘라낸다. 
"""
data.loc[행 이름:행 이름, 열 이름:열 이름]
"""
X1 = data.loc[:, 'Clump_Thickness' : 'Mitoses']
```

#### **df.iloc[]**

```python
# 주어진 행과 열부터, 주어진 행과 열의 그 전까지를 데이터 선택 후 잘라낸다. 
"""
data.iloc[행 인덱스:행 인덱스, 열 인덱스:열 인덱스]
"""

# 하나를 선택시에는 두 번 대괄호를 감싸야 데이터 프레임의 형태가 된다.  
Y = data.iloc[:, [-1]]
```

#### **pd.concat()**

```python
# pd.concat([df1, df2], axis = 1)
"""
options:
	- 중괄호 안에 병합할 데이터프레임 두 개를 지시
    - axis 
    	0 : 행 방향 결합
    	1 : 열 방향 결합
"""

Fvote = pd.concat([X1_dum, XY], axis = 1)
Fvote.head()
```



### 범주형 데이터

#### **df.replace()**

```python
# 주어진 값을 특정 값으로 대체
X1['gender'] = X1['gender'].replace([1, 2], ['male', 'female'])  
```

#### **pd.get_dummies(df)**

```python
# 범주변수를 one-hot-encoding으로 변환한다.  
# 모든 값이 0 또는 1로 변경되고, 1은 해당된다는 의미, 0은 해당되지 않는다는 의미가 된다.  
X1_dum = pd.get_dummies(X1)
X1_dum.head()
```



# 기계학습 모델  

## 단순선형회귀  

```python
import matplotlib as mpl
mpl.use("Agg")
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LinearRegression

def loss(x, y, beta_0, beta_1):
    N = len(x)
    loss = 0
    
    for i in range(len(x)):
        x_i, y_i = x[i], y[i]
        loss_value = (y_i - ((beta_0 * x_i) + beta_1)) ** 2
        loss += loss_value
        
    return loss
    
X = [8.70153760, 3.90825773, 1.89362433, 3.28730045, 7.39333004, 2.98984649, 2.25757240, 9.84450732, 9.94589513, 5.48321616]
Y = [5.64413093, 3.75876583, 3.87233310, 4.40990425, 6.43845020, 4.02827829, 2.26105955, 7.15768995, 6.29097441, 5.19692852]

train_X = np.array(X).reshape(-1, 1)
train_Y = np.array(Y)

'''
여기에서 모델을 트레이닝합니다.
'''
lrmodel = LinearRegression()
lrmodel.fit(train_X, train_Y)

'''
loss가 최소가 되는 직선의 기울기와 절편을 계산함
'''
beta_0 = lrmodel.coef_[0]   # lrmodel로 구한 직선의 기울기
beta_1 = lrmodel.intercept_ # lrmodel로 구한 직선의 y절편

print("beta_0: %f" % beta_0)
print("beta_1: %f" % beta_1)
print("Loss: %f" % loss(X, Y, beta_0, beta_1))

plt.scatter(X, Y) # (x, y) 점을 그립니다.
plt.plot([0, 10], [beta_1, 10 * beta_0 + beta_1], c='r') # y = beta_0 * x + beta_1 에 해당하는 선을 그립니다.

plt.xlim(0, 10) # 그래프의 X축을 설정합니다.
plt.ylim(0, 10) # 그래프의 Y축을 설정합니다.
plt.savefig("test.png") # 저장 후 엘리스에 이미지를 표시합니다.
eu.send_image("test.png")
```

## 다중선형회귀

```python
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

'''
./data/Advertising.csv 에서 데이터를 읽어, X와 Y를 만듭니다.

X는 (200, 3) 의 shape을 가진 2차원 np.array,
Y는 (200,) 의 shape을 가진 1차원 np.array여야 합니다.

X는 FB, TV, Newspaper column 에 해당하는 데이터를 저장해야 합니다.
Y는 Sales column 에 해당하는 데이터를 저장해야 합니다.
'''

import csv
csvreader = csv.reader(open("data/Advertising.csv"))

x = []
y = []

next(csvreader)

for line in csvreader :
    x_i = [ float(line[1]), float(line[2]), float(line[3]) ]
    y_i = float(line[4])
    x.append(x_i)
    y.append(y_i)

X = np.array(x)
Y = np.array(y)

lrmodel = LinearRegression()
lrmodel.fit(X, Y)

beta_0 = lrmodel.coef_[0] # 0번째 변수에 대한 계수 (페이스북)
beta_1 = lrmodel.coef_[1] # 1번째 변수에 대한 계수 (TV)
beta_2 = lrmodel.coef_[2] # 2번째 변수에 대한 계수 (신문)
beta_3 = lrmodel.intercept_ # y절편 (기본 판매량)

print("beta_0: %f" % beta_0)
print("beta_1: %f" % beta_1)
print("beta_2: %f" % beta_2)
print("beta_3: %f" % beta_3)

def expected_sales(fb, tv, newspaper, beta_0, beta_1, beta_2, beta_3):
    '''
    FB에 fb만큼, TV에 tv만큼, Newspaper에 newspaper 만큼의 광고비를 사용했고,
    트레이닝된 모델의 weight 들이 beta_0, beta_1, beta_2, beta_3 일 때
    예상되는 Sales 의 양을 출력합니다.
    '''
    sales = beta_0 * fb + beta_1 * tv + beta_2 * newspaper + beta_3
    
    return sales

print("예상 판매량: %f" % expected_sales(10, 12, 3, beta_0, beta_1, beta_2, beta_3))
```

# sklearn  

## model_selection

### **train_test_split()**

```python
# 데이터 셋 나눌 때 사용

from sklearn.model_selection import train_test_split  
	train_test_split(데이터 1, 데이터 2, stratify=대상데이터, random_state=42)
"""  
options 
	데이터 1 : X가 될 데이터 
	데이터 2 : Y가 될 데이터  
	stratify = 데이터 구분시 레이블의 범주비율에 맞게 설정  
	random_state = 분석할 때마다 다른 결과가 나오는 것을 막기 위해 설정
	train_size = 데이터를 나눌 비율 
		- default : 0.7
"""

from sklearn.model_selection import train_test_split  

X_train, X_test, y_train, y_test = train_test_split(X1, Y, stratify=Y, random_state=42)
```

### **cross_val_score()**

```python
# 랜덤 없는 교차검증  
# cross_val_score(model, X_train, y_train, cv = 5)

"""
options  
    model = 검증시킬 모델    
    X_train = 검증할 데이터 X
    y_train = 출력값 데이터 y
    cv  
"""

# train data 를 모두 한 번에 사용하지 않고 5개 그룹으로 나누어 이 중 한 그룹을 빼고 4개 그룹만 훈련을 한다. 이것을 5번 반복한다.
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X_train, y_train, cv=5)
print("5개 테스트 셋 정확도 : ", scores)
print("정확도 평균 : ", scores.mean())
```

### KFold

```python
# 랜덤 있는 교차검증 
"""
options 
	n_splits : 그룹의 수  
	shuffle : 섞기 여부  
	random_state : 분석할 때마다 다른 결과가 나오는 것을 막기 위해 설정
"""

from sklearn.model_selection import KFold  

kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# kfold 모델을 cross_val_score의 cv에 저장
score = cross_val_score(model, X_train, y_train, cv=kfold)
print("5개 폴드의 정확도 :", scores)
```

### ShuffleSplit

```python
# 임의분할 교차검증  
"""
훈련 데이터와 테스트 데이터를 구성할 때 다른 교차검증에 사용되었던 데이터도 랜덤으로 선택되게 하는 방법  
따라서 전체 데이터 중 일부는 훈련데이터 또는 테스트데이터 어디에서도 선택되지 않을 수도 있다.  
"""

from sklearn.model_selection import ShuffleSplit

shuffle_split = ShuffleSplit(test_size=0.5, train_size=0.5, random_state=42)

# cv에 ShuffleSplit을 넣는다.
score = cross_val_score(model, X_train, y_train, cv=shuffle_split)
print("교차검증 정확도 : ", scores)
```

### GridSearchCV

```python
# 그리드 탐색
# GridSearchCV(Model명(), param_grid, cv=5, return_train_score=True)
"""
분석자가 하이퍼파라미터의 특성값을 지정하고, 각각 모델에 적용하여 모델적합도를 비교하는 방법  

options :
    Model 명 : 사용하고자 하는 모델 명  
    param_grid : 설정한 그리드 서치
    cv : 교차검증 설정 
    return_train_score : 정확도 결과 표시 유무 (True or False)
"""

from sklearn.model_selection import GridSearchCV
param_grid = {'C' : [0.001, 0.01, 0.1, 1, 10, 100]}

from sklearn.linear_model import LogisticRegression
grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, return_train_score=True)
grid_search.fit(X_train, y_train)

# w정확도가 가장 높은 하이퍼파라미터가 무엇인지 확인  
print("Best Parameter: {}".format(grid_search.best_params_))
print(f"Best Cross-validity score : {grid_search.best_score_}")
print(f"Test set Score : {grid_search.score(X_test, y_test)}")
print(pd.DataFrame(grid_search.cv_results_))
```

### RandomizedSearchCV  

```python
# RandomSearch
from sklearn.model_selection import RandomizedSearchCV  

# 범위를 정하고 그 안에서 무작위로 C값을 찾기 위함  
from scipy.stats import randint  

# 그 범위를 0.001 ~ 100으로 해본다.  
param_distribs = {'C' : randint(low=0.001, high=100)}

# 알고리즘은 로지스틱 선형회귀르 사용  
from sklarn.linear_model import LogisticRegression

# random search를 로지스틱 모델에 적용하여 훈련데이터 학습  

"""
options
	모델 이름 :
	param_distributions : 매개변수 모음 (GridSearchCv의 paramgrid와 다름)
	cv : 교차검증 여부 (숫자 or 교차검증 모델)
	return_train_score = 훈련데이터 정확도 결과를 제시
"""
random_search = RandomizedSearchCV(LogisticRegression(), 
                                   param_distributions=param_distribs,
                                   cv=5, return_train_score=True)

# 베스트 매개변수를 제시  
random_search.best_params_  

# 베스트 매개변수 대입시의 정확도 확인 
random_search.best_score_

# 테스트 데이터에 최적 탐색 하이퍼파라미터 적용, 정확도 확인  
random_search.score(X_test, y_test)

# cv별 상세 결과값 확인  
pd.DataFrame(random_search.cv_results_)
```



## preprocessing

### **MinMaxScaler()**

```python
# MinMax 정규화 이용  
from sklearn.preprocessing import MinMaxScaler
scaler_minmax = MinMaxScaler()

# fit을 사용해서, 학습 데이터로 기준을 설정한다.
# 추후에 test 데이터를 정규화할 때도 train 데이터의 MinMax 기준을 적용해야 한다.  
scaler_minmax.fit(X_train)
X_scaled_minmax_train = scaler_minmax.transform(X_train)
```

### **StandardScaler()**

```python
# StandardScaler 정규화 이용  
from sklearn.preprocessing import StandardScaler
scaler_standard = StandardScaler()

scaler_standard.fit(X_train)
X_scaled_standard_train = scaler_standard.transform(X_train)
```

## decomposition  

### **PCA()**

```python
pca = sklearn.decomposition.PCA(num_components)
pca.fit(X)
pca_array = pca.transform(X)
```





## linear_model  

### **LogisticRegression()**

```python
# 로지스틱 선형 회귀 사용
from sklearn.linear_model import LogisticRegression

"""
options
	C : default = '1', 작을수록 모델이 단순해지고, 값이 커질수록 모델이 복잡해짐  
		C는 로그스케일 (0.01, 0.1, 1, 10, 100 등) 단위로 최적치 탐색 권고  
	solver 
		'sag' : 평균경사하강법을 적용하여 빠른 속도로 연산 가능 
"""


model = LogisticRegression()
model.fit(X_scaled_minmax_train, y_train)

# 모델명.predict(x_data) : 예측치 구하기  
pred_train = model.predict(X_scaled_minmax_train)

# 모델명.score(x_data, y_data) : 정확도 확인
model.score(X_scaled_minmax_train, y_train)
```

### **LinearRegression()**

```python
# 선형 회귀 모델 사용
from sklearn.linear_model import LinearRegression  

model = LinearRegression()
model.fit(X_scaled_minmax_train, y_train)

# 에측치 구하기
pred_train = model.predict(X_scaled_minmax_train)

# 정확도 확인
model.score(X_scaled_minmax_train, y_train)
```

### **BayesianRidge**()  

```python
from sklearn.linear_model import BayesianRidge  

model=BayesianRidge()
model.fit(X_scaled_train, Y_train)
pred_train = model.predict(X_scaled_train)
model.score(X_scaled_train, Y_train)
```





## neighbors  

### KNeighborsClassifier  

```python
"""
options  
	n_neighbors : K 군집 설정 (default : 5)
	metric : 거리측정법 (default : minkowski)
"""

KNN_model = KNeighborsClassifier()
KNN_model.fit(X_train, y_train)
print(f"KNN_model score => train : {KNN_model.score(X_train, y_train)}")
pred_train = KNN_model.predict(X_train) 
```

### KNeighborsRegressor

```python
from sklearn.neighbors import KNeighborsRegressor  
model = KNeighborsRegressor()
model.fit(X_scaled_train, y_train)
pred_train = model.predict(X_scaled_train)
```

## naive_bayes  

### GaussianNB  

```python
"""
options:
	var_smoothing: 안정적인 연산을 위해 분산에 더해지는 모든 특성치의 최대 분산 비율  (default=0.000000001)
"""

from sklearn.naive_bayes import GaussianNB  

model=GaussianNB()
model.fit(X_scaled_train, Y_train)
pred_train = model.predict(X_scaled_train)
model.score(X_scaled_train, Y_train)
```

## metrics  

### **confusion_matrix**()

```python
# 혼동행렬 라이브러리 임포트  
from sklearn.metrics import confusion_matrix  

# 실제 y_train 데이터와, 모델이 예측한 pred_train을 비교
confusion_train = confusion_matrix(y_train, pred_train)
print("훈련데이터 오차행렬 : \n", confusion_train)
```

### **classification_report()**

```python
from sklearn.metrics import classification_report  

cfreport_train = classification_report(y_train, pred_train)

# 정확도 외에 정밀도(precision), 재현율(recall), f1-score 등이 제시된다.
print("분류예측 레포트 : \n", cfreport_train)
```

