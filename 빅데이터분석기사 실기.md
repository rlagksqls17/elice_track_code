# Python  



# Numpy

#### import 

 ```python
 import numpy as np  
 ```

#### array

```python
A = np.array([[1, 2], [3, 4]])
```

### 연산

```python
A = np.array([[1, 2], [3, 4]])

# 일반 연산
print(A * 3)
print(A + A)
print(A - A)
print(A ** 2)
print(3 ** A)
print(A * A)


# 비교 연산 (같은 차원에서의 연산)
a = np.array([1, 2, 3, 4])
b = np.array([4, 2, 2, 4])

print(a == b)
>> [False, True, False, True]

print(a > b)
>> [False, False, True, False]


# 논리 연산  
a = np.array([1, 1, 0, 0], dtype=bool)
b = np.array([1, 0, 1, 0], dtype=bool)

np.logical_or(a, b)
>> [True, True, True, False]

np.logical_and(a, b)
>> [True, False, False, False]
```

#### **np.dot(x,y)**

```python
# 내적
x = np.array([[1, 2], [3, 4]])
y = np.array([[3, 4], [3, 2]])

print(np.dot(x, y))
```

#### **np.transpose(A)**

```python
A = np.array([[1, 2, 3], [4, 5, 6]])
print(A.T)

>> [[1 4]
   [2 5]
   [3 6]]
```

#### **np.linalg.inv(A)**

```python
A = np.array([[1, 2], [3, 4]])
print(np.linalg.inv(A))
```



#### Reductions

```python
a = np.array([1, 2, 3, 4, 5])

np.sum(a)
>> 15

a.sum()
>> 15

a.min()
>> 1

a.max()
>> 5

a.argmin()
>> 0

a.argmax()
>> 4
```

#### Logical Reductions  

```python
a = np.array([True, True, True])
b = np.array([True, True, False])


# np.all(a) : Array 내의 모든 값이 True인가?
np.all(a)
>> True

np.all(b)
>> False

# np.any(a) : Array 내의 값이 하나라도 True인가?
np.any(a)
>> True

np.any(b)
>> True
```

#### Statistical Reductions  

```python
x = np.array([1, 2, 3, 1])

# np.mean(x) : 평균값
# np.median(x) : 중간값  
# np.std(x) : 표준편차
# np.var(x) : 분산
```



# Pandas  

## 파일 저장, 불러오기

### **pd.read_csv()**

```python
# 지정된 경로의 파일 이름을 가진 파일을 불러옴

"""
pd.read_csv()
	'파일 경로' : 불러올 파일의 경로와 이름 
	encoding : 부호화할 알고리즘 방식
		- 'utf-8' : 한글 인코딩 1
		- 'euc-kr' : 한글 인코딩 2
"""

data = pd.read_csv('/content/data/breast-cancser.csv', encoding='utf-8')
```

## 데이터 확인  

### 전체 데이터  

#### **df.shape()**

```python
# 전체 데이터의 열과 행을 확인할 수 있음
print(data.shape)
>> (683, 11)
```

df.describe()

```python
# 전체 데이터의 통계량을 요약해서 보여줌  
pd.DataFrame(X_scaled_minmax_train).describe()
```



### 범주형 데이터  

#### df.value_counts()

```python
# 해당 레이블의 변수 빈도를 보여준다.  
# 컬럼을 지정해서 사용할 경우 해당 컬럼에서의 레이블 변수 빈도를 보여준다.  

"""
df['Columns_name'].value_counts()
	sort : 값의 정렬 여부
		- False : 정렬 X
		- True : 값을 오름차순으로 정렬해줌
"""

daat['Class'].value_counts(sort=False)
>> 0    444
   1    239
   Name: Class, dtype: int64
```

## 데이터 전처리  

### 전체 데이터

#### **df.loc[]**

```python
# 주어진 행과 열부터, 주어진 행과 열을 포함해서 데이터를 선택한 후 잘라낸다. 
"""
data.loc[행 이름:행 이름, 열 이름:열 이름]
"""
X1 = data.loc[:, 'Clump_Thickness' : 'Mitoses']
```

#### **df.iloc[]**

```python
# 주어진 행과 열부터, 주어진 행과 열의 그 전까지를 데이터 선택 후 잘라낸다. 
"""
data.iloc[행 인덱스:행 인덱스, 열 인덱스:열 인덱스]
"""

# 하나를 선택시에는 두 번 대괄호를 감싸야 데이터 프레임의 형태가 된다.  
Y = data.iloc[:, [-1]]
```

#### **pd.concat()**

```python
# pd.concat([df1, df2], axis = 1)
"""
options:
	- 중괄호 안에 병합할 데이터프레임 두 개를 지시
    - axis 
    	0 : 행 방향 결합
    	1 : 열 방향 결합
"""

Fvote = pd.concat([X1_dum, XY], axis = 1)
Fvote.head()
```



### 범주형 데이터

#### **df.replace()**

```python
# 주어진 값을 특정 값으로 대체
X1['gender'] = X1['gender'].replace([1, 2], ['male', 'female'])  
```

#### **pd.get_dummies(df)**

```python
# 범주변수를 one-hot-encoding으로 변환한다.  
# 모든 값이 0 또는 1로 변경되고, 1은 해당된다는 의미, 0은 해당되지 않는다는 의미가 된다.  
X1_dum = pd.get_dummies(X1)
X1_dum.head()
```



# 기계학습 모델  

## 단순선형회귀  

```python
import matplotlib as mpl
mpl.use("Agg")
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LinearRegression

def loss(x, y, beta_0, beta_1):
    N = len(x)
    loss = 0
    
    for i in range(len(x)):
        x_i, y_i = x[i], y[i]
        loss_value = (y_i - ((beta_0 * x_i) + beta_1)) ** 2
        loss += loss_value
        
    return loss
    
X = [8.70153760, 3.90825773, 1.89362433, 3.28730045, 7.39333004, 2.98984649, 2.25757240, 9.84450732, 9.94589513, 5.48321616]
Y = [5.64413093, 3.75876583, 3.87233310, 4.40990425, 6.43845020, 4.02827829, 2.26105955, 7.15768995, 6.29097441, 5.19692852]

train_X = np.array(X).reshape(-1, 1)
train_Y = np.array(Y)

'''
여기에서 모델을 트레이닝합니다.
'''
lrmodel = LinearRegression()
lrmodel.fit(train_X, train_Y)

'''
loss가 최소가 되는 직선의 기울기와 절편을 계산함
'''
beta_0 = lrmodel.coef_[0]   # lrmodel로 구한 직선의 기울기
beta_1 = lrmodel.intercept_ # lrmodel로 구한 직선의 y절편

print("beta_0: %f" % beta_0)
print("beta_1: %f" % beta_1)
print("Loss: %f" % loss(X, Y, beta_0, beta_1))

plt.scatter(X, Y) # (x, y) 점을 그립니다.
plt.plot([0, 10], [beta_1, 10 * beta_0 + beta_1], c='r') # y = beta_0 * x + beta_1 에 해당하는 선을 그립니다.

plt.xlim(0, 10) # 그래프의 X축을 설정합니다.
plt.ylim(0, 10) # 그래프의 Y축을 설정합니다.
plt.savefig("test.png") # 저장 후 엘리스에 이미지를 표시합니다.
eu.send_image("test.png")
```

## 다중선형회귀

```python
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

'''
./data/Advertising.csv 에서 데이터를 읽어, X와 Y를 만듭니다.

X는 (200, 3) 의 shape을 가진 2차원 np.array,
Y는 (200,) 의 shape을 가진 1차원 np.array여야 합니다.

X는 FB, TV, Newspaper column 에 해당하는 데이터를 저장해야 합니다.
Y는 Sales column 에 해당하는 데이터를 저장해야 합니다.
'''

import csv
csvreader = csv.reader(open("data/Advertising.csv"))

x = []
y = []

next(csvreader)

for line in csvreader :
    x_i = [ float(line[1]), float(line[2]), float(line[3]) ]
    y_i = float(line[4])
    x.append(x_i)
    y.append(y_i)

X = np.array(x)
Y = np.array(y)

lrmodel = LinearRegression()
lrmodel.fit(X, Y)

beta_0 = lrmodel.coef_[0] # 0번째 변수에 대한 계수 (페이스북)
beta_1 = lrmodel.coef_[1] # 1번째 변수에 대한 계수 (TV)
beta_2 = lrmodel.coef_[2] # 2번째 변수에 대한 계수 (신문)
beta_3 = lrmodel.intercept_ # y절편 (기본 판매량)

print("beta_0: %f" % beta_0)
print("beta_1: %f" % beta_1)
print("beta_2: %f" % beta_2)
print("beta_3: %f" % beta_3)

def expected_sales(fb, tv, newspaper, beta_0, beta_1, beta_2, beta_3):
    '''
    FB에 fb만큼, TV에 tv만큼, Newspaper에 newspaper 만큼의 광고비를 사용했고,
    트레이닝된 모델의 weight 들이 beta_0, beta_1, beta_2, beta_3 일 때
    예상되는 Sales 의 양을 출력합니다.
    '''
    sales = beta_0 * fb + beta_1 * tv + beta_2 * newspaper + beta_3
    
    return sales

print("예상 판매량: %f" % expected_sales(10, 12, 3, beta_0, beta_1, beta_2, beta_3))
```

# sklearn  

## model_selection

### **train_test_split()**

```python
# 데이터 셋 나눌 때 사용

from sklearn.model_selection import train_test_split  
	train_test_split(데이터 1, 데이터 2, stratify=대상데이터, random_state=42)
"""  
options 
	데이터 1 : X가 될 데이터 
	데이터 2 : Y가 될 데이터  
	stratify = 데이터 구분시 레이블의 범주비율에 맞게 설정  
	random_state = 분석할 때마다 다른 결과가 나오는 것을 막기 위해 설정
	train_size = 데이터를 나눌 비율 
		- default : 0.7
"""

from sklearn.model_selection import train_test_split  

X_train, X_test, y_train, y_test = train_test_split(X1, Y, stratify=Y, random_state=42)
```

## preprocessing

### **MinMaxScaler()**

```python
# MinMax 정규화 이용  
from sklearn.preprocessing import MinMaxScaler
scaler_minmax = MinMaxScaler()

# fit을 사용해서, 학습 데이터로 기준을 설정한다.
# 추후에 test 데이터를 정규화할 때도 train 데이터의 MinMax 기준을 적용해야 한다.  
scaler_minmax.fit(X_train)
X_scaled_minmax_train = scaler_minmax.transform(X_train)
```

### **StandardScaler()**

```python
# StandardScaler 정규화 이용  
from sklearn.preprocessing import StandardScaler
scaler_standard = StandardScaler()

scaler_standard.fit(X_train)
X_scaled_standard_train = scaler_standard.transform(X_train)
```

## metrics  

### **confusion_matrix**()

```python
# 혼동행렬 라이브러리 임포트  
from sklearn.metrics import confusion_matrix  

# 실제 y_train 데이터와, 모델이 예측한 pred_train을 비교
confusion_train = confusion_matrix(y_train, pred_train)
print("훈련데이터 오차행렬 : \n", confusion_train)
```

### **classification_report()**

```python
from sklearn.metrics import classification_report  

cfreport_train = classification_report(y_train, pred_train)

# 정확도 외에 정밀도(precision), 재현율(recall), f1-score 등이 제시된다.
print("분류예측 레포트 : \n", cfreport_train)
```



## linear_model  

### **LogisticRegression()**

```python
# 로지스틱 선형 회귀 사용
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_scaled_minmax_train, y_train)

# 모델명.predict(x_data) : 예측치 구하기  
pred_train = model.predict(X_scaled_minmax_train)

# 모델명.score(x_data, y_data) : 정확도 확인
model.score(X_scaled_minmax_train, y_train)
```

### **LinearRegression()**

```python
# 선형 회귀 모델 사용
from sklearn.linear_model import LinearRegression  

model = LinearRegression()
model.fit(X_scaled_minmax_train, y_train)

# 에측치 구하기
pred_train = model.predict(X_scaled_minmax_train)

# 정확도 확인
model.score(X_scaled_minmax_train, y_train)
```

