# Python  



# Numpy

#### import 

 ```python
 import numpy as np  
 ```

#### array

```python
A = np.array([[1, 2], [3, 4]])
```

### 연산

```python
A = np.array([[1, 2], [3, 4]])

# 일반 연산
print(A * 3)
print(A + A)
print(A - A)
print(A ** 2)
print(3 ** A)
print(A * A)


# 비교 연산 (같은 차원에서의 연산)
a = np.array([1, 2, 3, 4])
b = np.array([4, 2, 2, 4])

print(a == b)
>> [False, True, False, True]

print(a > b)
>> [False, False, True, False]


# 논리 연산  
a = np.array([1, 1, 0, 0], dtype=bool)
b = np.array([1, 0, 1, 0], dtype=bool)

np.logical_or(a, b)
>> [True, True, True, False]

np.logical_and(a, b)
>> [True, False, False, False]
```

#### **np.dot(x,y)**

```python
# 내적
x = np.array([[1, 2], [3, 4]])
y = np.array([[3, 4], [3, 2]])

print(np.dot(x, y))
```

#### **np.transpose(A)**

```python
A = np.array([[1, 2, 3], [4, 5, 6]])
print(A.T)

>> [[1 4]
   [2 5]
   [3 6]]
```

#### **np.linalg.inv(A)**

```python
A = np.array([[1, 2], [3, 4]])
print(np.linalg.inv(A))
```



#### Reductions

```python
a = np.array([1, 2, 3, 4, 5])

np.sum(a)
>> 15

a.sum()
>> 15

a.min()
>> 1

a.max()
>> 5

a.argmin()
>> 0

a.argmax()
>> 4
```

#### Logical Reductions  

```python
a = np.array([True, True, True])
b = np.array([True, True, False])


# np.all(a) : Array 내의 모든 값이 True인가?
np.all(a)
>> True

np.all(b)
>> False

# np.any(a) : Array 내의 값이 하나라도 True인가?
np.any(a)
>> True

np.any(b)
>> True
```

#### Statistical Reductions  

```python
x = np.array([1, 2, 3, 1])

# np.mean(x) : 평균값
# np.median(x) : 중간값  
# np.std(x) : 표준편차
# np.var(x) : 분산
```



# Pandas  

## 파일 저장, 불러오기

### **pd.read_csv()**

```python
# 지정된 경로의 파일 이름을 가진 파일을 불러옴

"""
pd.read_csv()
	'파일 경로' : 불러올 파일의 경로와 이름 
	encoding : 부호화할 알고리즘 방식
		- 'utf-8' : 한글 인코딩 1
		- 'euc-kr' : 한글 인코딩 2
"""

data = pd.read_csv('/content/data/breast-cancser.csv', encoding='utf-8')
```

## 데이터 확인  

### 전체 데이터  

#### **df.shape()**

```python
# 전체 데이터의 열과 행을 확인할 수 있음
print(data.shape)
>> (683, 11)
```

df.describe()

```python
# 전체 데이터의 통계량을 요약해서 보여줌  
pd.DataFrame(X_scaled_minmax_train).describe()
```



### 범주형 데이터  

#### df.value_counts()

```python
# 해당 레이블의 변수 빈도를 보여준다.  
# 컬럼을 지정해서 사용할 경우 해당 컬럼에서의 레이블 변수 빈도를 보여준다.  

"""
df['Columns_name'].value_counts()
	sort : 값의 정렬 여부
		- False : 정렬 X
		- True : 값을 오름차순으로 정렬해줌
"""

daat['Class'].value_counts(sort=False)
>> 0    444
   1    239
   Name: Class, dtype: int64
```

## 데이터 전처리  

### 전체 데이터

#### **df.loc[]**

```python
# 주어진 행과 열부터, 주어진 행과 열을 포함해서 데이터를 선택한 후 잘라낸다. 
"""
data.loc[행 이름:행 이름, 열 이름:열 이름]
"""
X1 = data.loc[:, 'Clump_Thickness' : 'Mitoses']
```

#### **df.iloc[]**

```python
# 주어진 행과 열부터, 주어진 행과 열의 그 전까지를 데이터 선택 후 잘라낸다. 
"""
data.iloc[행 인덱스:행 인덱스, 열 인덱스:열 인덱스]
"""

# 하나를 선택시에는 두 번 대괄호를 감싸야 데이터 프레임의 형태가 된다.  
Y = data.iloc[:, [-1]]
```

#### **pd.concat()**

```python
# pd.concat([df1, df2], axis = 1)
"""
options:
	- 중괄호 안에 병합할 데이터프레임 두 개를 지시
    - axis 
    	0 : 행 방향 결합
    	1 : 열 방향 결합
"""

Fvote = pd.concat([X1_dum, XY], axis = 1)
Fvote.head()
```



### 범주형 데이터

#### **df.replace()**

```python
# 주어진 값을 특정 값으로 대체
X1['gender'] = X1['gender'].replace([1, 2], ['male', 'female'])  
```

#### **pd.get_dummies(df)**

```python
# 범주변수를 one-hot-encoding으로 변환한다.  
# 모든 값이 0 또는 1로 변경되고, 1은 해당된다는 의미, 0은 해당되지 않는다는 의미가 된다.  
X1_dum = pd.get_dummies(X1)
X1_dum.head()
```



# 기계학습 모델  

## 단순선형회귀  

```python
import matplotlib as mpl
mpl.use("Agg")
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LinearRegression

def loss(x, y, beta_0, beta_1):
    N = len(x)
    loss = 0
    
    for i in range(len(x)):
        x_i, y_i = x[i], y[i]
        loss_value = (y_i - ((beta_0 * x_i) + beta_1)) ** 2
        loss += loss_value
        
    return loss
    
X = [8.70153760, 3.90825773, 1.89362433, 3.28730045, 7.39333004, 2.98984649, 2.25757240, 9.84450732, 9.94589513, 5.48321616]
Y = [5.64413093, 3.75876583, 3.87233310, 4.40990425, 6.43845020, 4.02827829, 2.26105955, 7.15768995, 6.29097441, 5.19692852]

train_X = np.array(X).reshape(-1, 1)
train_Y = np.array(Y)

'''
여기에서 모델을 트레이닝합니다.
'''
lrmodel = LinearRegression()
lrmodel.fit(train_X, train_Y)

'''
loss가 최소가 되는 직선의 기울기와 절편을 계산함
'''
beta_0 = lrmodel.coef_[0]   # lrmodel로 구한 직선의 기울기
beta_1 = lrmodel.intercept_ # lrmodel로 구한 직선의 y절편

print("beta_0: %f" % beta_0)
print("beta_1: %f" % beta_1)
print("Loss: %f" % loss(X, Y, beta_0, beta_1))

plt.scatter(X, Y) # (x, y) 점을 그립니다.
plt.plot([0, 10], [beta_1, 10 * beta_0 + beta_1], c='r') # y = beta_0 * x + beta_1 에 해당하는 선을 그립니다.

plt.xlim(0, 10) # 그래프의 X축을 설정합니다.
plt.ylim(0, 10) # 그래프의 Y축을 설정합니다.
plt.savefig("test.png") # 저장 후 엘리스에 이미지를 표시합니다.
eu.send_image("test.png")
```

## 다중선형회귀

```python
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

'''
./data/Advertising.csv 에서 데이터를 읽어, X와 Y를 만듭니다.

X는 (200, 3) 의 shape을 가진 2차원 np.array,
Y는 (200,) 의 shape을 가진 1차원 np.array여야 합니다.

X는 FB, TV, Newspaper column 에 해당하는 데이터를 저장해야 합니다.
Y는 Sales column 에 해당하는 데이터를 저장해야 합니다.
'''

import csv
csvreader = csv.reader(open("data/Advertising.csv"))

x = []
y = []

next(csvreader)

for line in csvreader :
    x_i = [ float(line[1]), float(line[2]), float(line[3]) ]
    y_i = float(line[4])
    x.append(x_i)
    y.append(y_i)

X = np.array(x)
Y = np.array(y)

lrmodel = LinearRegression()
lrmodel.fit(X, Y)

beta_0 = lrmodel.coef_[0] # 0번째 변수에 대한 계수 (페이스북)
beta_1 = lrmodel.coef_[1] # 1번째 변수에 대한 계수 (TV)
beta_2 = lrmodel.coef_[2] # 2번째 변수에 대한 계수 (신문)
beta_3 = lrmodel.intercept_ # y절편 (기본 판매량)

print("beta_0: %f" % beta_0)
print("beta_1: %f" % beta_1)
print("beta_2: %f" % beta_2)
print("beta_3: %f" % beta_3)

def expected_sales(fb, tv, newspaper, beta_0, beta_1, beta_2, beta_3):
    '''
    FB에 fb만큼, TV에 tv만큼, Newspaper에 newspaper 만큼의 광고비를 사용했고,
    트레이닝된 모델의 weight 들이 beta_0, beta_1, beta_2, beta_3 일 때
    예상되는 Sales 의 양을 출력합니다.
    '''
    sales = beta_0 * fb + beta_1 * tv + beta_2 * newspaper + beta_3
    
    return sales

print("예상 판매량: %f" % expected_sales(10, 12, 3, beta_0, beta_1, beta_2, beta_3))
```

# sklearn  

## model_selection

### **train_test_split()**

```python
# 데이터 셋 나눌 때 사용

from sklearn.model_selection import train_test_split  
	train_test_split(데이터 1, 데이터 2, stratify=대상데이터, random_state=42)
"""  
options 
	데이터 1 : X가 될 데이터 
	데이터 2 : Y가 될 데이터  
	stratify = 데이터 구분시 레이블의 범주비율에 맞게 설정  
	random_state = 분석할 때마다 다른 결과가 나오는 것을 막기 위해 설정
	train_size = 데이터를 나눌 비율 
		- default : 0.7
"""

from sklearn.model_selection import train_test_split  

X_train, X_test, y_train, y_test = train_test_split(X1, Y, stratify=Y, random_state=42)
```

### **cross_val_score()**

```python
# 랜덤 없는 교차검증  
# cross_val_score(model, X_train, y_train, cv = 5)

"""
options  
    model = 검증시킬 모델    
    X_train = 검증할 데이터 X
    y_train = 출력값 데이터 y
    cv  
"""

# train data 를 모두 한 번에 사용하지 않고 5개 그룹으로 나누어 이 중 한 그룹을 빼고 4개 그룹만 훈련을 한다. 이것을 5번 반복한다.
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X_train, y_train, cv=5)
print("5개 테스트 셋 정확도 : ", scores)
print("정확도 평균 : ", scores.mean())
```

### KFold

```python
# 랜덤 있는 교차검증 
"""
options 
	n_splits : 그룹의 수  
	shuffle : 섞기 여부  
	random_state : 분석할 때마다 다른 결과가 나오는 것을 막기 위해 설정
"""

from sklearn.model_selection import KFold  

kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# kfold 모델을 cross_val_score의 cv에 저장
score = cross_val_score(model, X_train, y_train, cv=kfold)
print("5개 폴드의 정확도 :", scores)
```

### ShuffleSplit

```python
# 임의분할 교차검증  
"""
훈련 데이터와 테스트 데이터를 구성할 때 다른 교차검증에 사용되었던 데이터도 랜덤으로 선택되게 하는 방법  
따라서 전체 데이터 중 일부는 훈련데이터 또는 테스트데이터 어디에서도 선택되지 않을 수도 있다.  
"""

from sklearn.model_selection import ShuffleSplit

shuffle_split = ShuffleSplit(test_size=0.5, train_size=0.5, random_state=42)

# cv에 ShuffleSplit을 넣는다.
score = cross_val_score(model, X_train, y_train, cv=shuffle_split)
print("교차검증 정확도 : ", scores)
```

### GridSearchCV

```python
# 그리드 탐색
# GridSearchCV(Model명(), param_grid, cv=5, return_train_score=True)
"""
분석자가 하이퍼파라미터의 특성값을 지정하고, 각각 모델에 적용하여 모델적합도를 비교하는 방법  

options :
    Model 명 : 사용하고자 하는 모델 명  
    param_grid : 설정한 그리드 서치
    cv : 교차검증 설정 
    return_train_score : 정확도 결과 표시 유무 (True or False)
"""

from sklearn.model_selection import GridSearchCV
param_grid = {'C' : [0.001, 0.01, 0.1, 1, 10, 100]}

from sklearn.linear_model import LogisticRegression
grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, return_train_score=True)
grid_search.fit(X_train, y_train)

# w정확도가 가장 높은 하이퍼파라미터가 무엇인지 확인  
print("Best Parameter: {}".format(grid_search.best_params_))
print(f"Best Cross-validity score : {grid_search.best_score_}")
print(f"Test set Score : {grid_search.score(X_test, y_test)}")
print(pd.DataFrame(grid_search.cv_results_))
```

### RandomizedSearchCV  

```python
# RandomSearch
from sklearn.model_selection import RandomizedSearchCV  

# 범위를 정하고 그 안에서 무작위로 C값을 찾기 위함  
from scipy.stats import randint  

# 그 범위를 0.001 ~ 100으로 해본다.  
param_distribs = {'C' : randint(low=0.001, high=100)}

# 알고리즘은 로지스틱 선형회귀르 사용  
from sklarn.linear_model import LogisticRegression

# random search를 로지스틱 모델에 적용하여 훈련데이터 학습  

"""
options
	모델 이름 :
	param_distributions : 매개변수 모음 (GridSearchCv의 paramgrid와 다름)
	cv : 교차검증 여부 (숫자 or 교차검증 모델)
	return_train_score = 훈련데이터 정확도 결과를 제시
"""
random_search = RandomizedSearchCV(LogisticRegression(), 
                                   param_distributions=param_distribs,
                                   cv=5, return_train_score=True)

# 베스트 매개변수를 제시  
random_search.best_params_  

# 베스트 매개변수 대입시의 정확도 확인 
random_search.best_score_

# 테스트 데이터에 최적 탐색 하이퍼파라미터 적용, 정확도 확인  
random_search.score(X_test, y_test)

# cv별 상세 결과값 확인  
pd.DataFrame(random_search.cv_results_)
```



## preprocessing

### **MinMaxScaler()**

```python
# MinMax 정규화 이용  
from sklearn.preprocessing import MinMaxScaler
scaler_minmax = MinMaxScaler()

# fit을 사용해서, 학습 데이터로 기준을 설정한다.
# 추후에 test 데이터를 정규화할 때도 train 데이터의 MinMax 기준을 적용해야 한다.  
scaler_minmax.fit(X_train)
X_scaled_minmax_train = scaler_minmax.transform(X_train)
```

### **StandardScaler()**

```python
# StandardScaler 정규화 이용  
from sklearn.preprocessing import StandardScaler
scaler_standard = StandardScaler()

scaler_standard.fit(X_train)
X_scaled_standard_train = scaler_standard.transform(X_train)
```

### LabelEncoder

```python
from sklearn.preprocessing import LabelEncoder  

x_train.loc[:,['주구매상품', '주구매지점']] = x_train.loc[:,['주구매상품', '주구매지점']].apply(LabelEncoder().fit_transform)
```



## decomposition  

### **PCA()**

```python
pca = sklearn.decomposition.PCA(num_components)
pca.fit(X)
pca_array = pca.transform(X)
```





## linear_model  

### **LogisticRegression()**

```python
# 로지스틱 선형 회귀 사용
from sklearn.linear_model import LogisticRegression

"""
options
	C : default = '1', 작을수록 모델이 단순해지고, 값이 커질수록 모델이 복잡해짐  
		C는 로그스케일 (0.01, 0.1, 1, 10, 100 등) 단위로 최적치 탐색 권고  
	solver 
		'sag' : 평균경사하강법을 적용하여 빠른 속도로 연산 가능 
"""


model = LogisticRegression()
model.fit(X_scaled_minmax_train, y_train)

# 모델명.predict(x_data) : 예측치 구하기  
pred_train = model.predict(X_scaled_minmax_train)

# 모델명.score(x_data, y_data) : 정확도 확인
model.score(X_scaled_minmax_train, y_train)
```

### **LinearRegression()**

```python
# 선형 회귀 모델 사용
from sklearn.linear_model import LinearRegression  

model = LinearRegression()
model.fit(X_scaled_minmax_train, y_train)

# 에측치 구하기
pred_train = model.predict(X_scaled_minmax_train)

# 정확도 확인
model.score(X_scaled_minmax_train, y_train)
```

### **BayesianRidge**()  

```python
from sklearn.linear_model import BayesianRidge  

model=BayesianRidge()
model.fit(X_scaled_train, Y_train)
pred_train = model.predict(X_scaled_train)
model.score(X_scaled_train, Y_train)
```





## neighbors  

### KNeighborsClassifier  

```python
"""
options  
	n_neighbors : K 군집 설정 (default : 5)
	metric : 거리측정법 (default : minkowski)
"""

KNN_model = KNeighborsClassifier()
KNN_model.fit(X_train, y_train)
print(f"KNN_model score => train : {KNN_model.score(X_train, y_train)}")
pred_train = KNN_model.predict(X_train) 
```

### KNeighborsRegressor

```python
from sklearn.neighbors import KNeighborsRegressor  
model = KNeighborsRegressor()
model.fit(X_scaled_train, y_train)
pred_train = model.predict(X_scaled_train)
```

## naive_bayes  

### GaussianNB  

```python
"""
options:
	var_smoothing: 안정적인 연산을 위해 분산에 더해지는 모든 특성치의 최대 분산 비율  (default=0.000000001)
"""

from sklearn.naive_bayes import GaussianNB  

model=GaussianNB()
model.fit(X_scaled_train, Y_train)
pred_train = model.predict(X_scaled_train)
model.score(X_scaled_train, Y_train)
```

## neural_network  

### **MLPClassifier()**

```python
"""
options:
	hidden_layer_sizes: 은닉층 수  
	solver: 옵티마이저  
		- sgd
		- adam
	activation: 활성화함수
		- tanh
		- relu
"""


# 인공신경망 모델 적용  
from sklearn.neural_network import MLPClassifier  

model = MLPClassifier()
model.fit(X_scaled_train, Y_train)
pred_train = model.predict(X_scaled_train)
print(f"훈련데이터 스코어 {model.score(X_scaled_train, Y_train)}\n")
```

### **MLPRegressor()**

```python
from sklearn.neural_network import MLPRegressor  

model = MLPRegressor()
model.fit(X_scaled_train, y_train)
print(f"\n훈련 데이터 정확도 : {model.score(X_scaled_train, y_train)}")
print(f"테스트 데이터 정확도 : {model.score(X_scaled_test, y_test)}")

pred_train = model.predict(X_scaled_train)
pred_test = model.predict(X_scaled_test)
```

## svm

### 분류 - **SVC()**

```python
from sklearn.svm import SVC  

"""
options :
	kernel : 집단을 분류하거나 값을 예측하는 함수에 대한 정의 
	- "rbf", "linear", "poly", "sigmoid", "precomputed"
	C : default = '1', 작을수록 모델이 단순해지고, 값이 커질수록 모델이 복잡해짐  
	- C는 로그스케일 (0.01, 0.1, 1, 10, 100 등) 단위로 최적치 탐색 권고  
	gamma : ?
"""

model = SVC()
model.fit(X_scaled_train, y_train)
pred_train = model.predict(X_scaled_train)
model.score(X_scaled_train, y_train)

"""
종합 정리 
서포트 벡터머신은 다른 알고리즘에 비해 kernel 종류, C와 gamma 등 하이퍼파라미터가 다양하다.  
그만큼 모델의 유연성이 뛰어나다.  
다만 이에 대한 이해가 깊어야 모델을 유연하게 다룰 수 있다.  
그러나 대규모 데이터에서는 다소 느린 학습속도를 보인다.
"""
```

### **회귀 - SVR()**

```python
from sklearn.svm import SVR  

# 회귀에서는 kernel='poly'가 잘 맞고 다른 곳에서는 잘 맞지 않음  
model = SVR(kernel='poly')
model.fit(X_scaled_train, y_train)
pred_train = model.predict(X_scaled_train)
model.score(X_scaled_train, y_train)

"""
종합 정리  
서포트 벡터머신의 회귀 알고리즘인 SVR은 kernel에 민감하다. 따라서 5가지 kernel 종류 중 우선 가장 맞는 것을 선정해야 한다. 그리고 그리드탐색과 랜덤탐색의 시간이 데이터가 많을수록 상당히 오래 걸린다.  
이러한 단점으로 인해 많은 데이터에서 적용하기에는 무리가 있다. 
"""
```

## tree  

```html
의사결정나무는 직관적으로 결과를 도식화하여 볼 수 있으며, 어떻게 분류되는지를 알 수 있다는 장점이 있다.  
그러나 분류되는 단계가 많을수록 이해하기 어려우며 데이터에 따라 결과가 안정적이지 못하여 실제 머신러닝 알고리즘으로 많이 활용되지는 않는다. 그러나 특성치가 많지 않고, 최종 알고리즘을 도출하기 전에 탐색적으로 주요한 분류 변수가 어떤 것인지를 확인하는 차원에서는 활용하기 좋다.  
```



### 분류 - DecisionTreeClassifier  

```python
from sklearn.tree import DecisionTreeClassifier  

"""
종합정리 
의사결정 나무는 학습데이터에 매우 과적합되는 경향이 있다.  
100 %의 분류 예측이 이를 보여준다. 또한 하이퍼파라미터의 설정에 대한 기준 역시 논리적으로 설정하기 어렵다.  
이러한 한계에도 불구하고 기본적 방향을 탐색하기 위한 초기 분석모델로서는 유용하다.  
실제 분석에서는 의사결정나무의 앙상블인 랜덤포레스트를 더 선호한다.
"""
```

### 회귀-DecisionTreeRegressor  

```python
from sklearn.tree import DecisionTreeRegressor  

"""
종합정리  
회귀문제에서도 기본모델에서는 과대적합되는 경향을 보였다.  
그러나 적절한 하이퍼파라미터를 찾으면 테스트 데이터셋의 결과가 양호하게 나와 일반화 가능성도 높다.  
다만 기본 디폴트 모델이 아닌 데이터에 적절한 설정을 찾아야 한다.  
"""
```

## ensemble

### 분류 - RandomForestClassifier

```python
from sklearn.ensemble import RandomForestClassifier  
```

### 회귀 - RandomForestRegressor  

```python
from sklearn.ensemble import RandomForestRegressor  

"""
종합정리 :
기본 디폴트 모델에서는 과대적합되는 경향을 보였다.  
그러나 적절한 모델수와 특성치를 탐색한 결과는 좋은 결과를 보였다.  
기본 설정보다는 최적의 하이퍼파라미터를 찾아 회귀문제에 적용하는 것이 적절한 모델이다.  
개별 알고리즘보다 더 좋은 성능을 보이며, 다른 앙상블 기법에 비해서도 심플하고 강력하다.  
그래서 실전 분석에서 가장 많이 사용하는 것이 랜덤포레스트이다. 
"""
```

### 분류-VotingClassifier  

```python
"""
options 
	estimators : 투표 후보 모델 
		- 'lr' : 로지스틱회귀 모델  
		- 'rf' : 랜덤포레스트 모델  
		- 'svc' : 서포트벡터머신 모델
	voting : 투표의 방식
		- 'hard' : 범주 투표기반 앙상블 
		- 'soft' : 확률 투표기반 앙상블
"""


"""
투표기반 앙상블을 사용하기 위해서는 먼저 개별 머신러닝 알고리즘을 결정해야 한다.  
여기서는 랜덤포레스트, 로지스틱모델, SVM 3가지를 사용해보려 한다.  
"""

from sklearn.ensemble import RandomForestClassifier  
from sklearn.linear_model import LogisticRegression  
from sklearn.svm import SVC  
from sklearn.ensemble import VotingClassifier  

logit_model = LogisticRegression(random_state=42)
rnf_model = RandomForestClassifier(random_state=42)
svm_model = SVC(random_state=42)  

voting_hard = VotingClassifier(estimators=[('lr', logit_model), ('rf', rnf_model), ('svc', svm_model)],
                              voting='hard')

voting_hard.fit(X_scaled_train, y_train)

"""
일반적으로 앙상블은 하드보팅 보다는 소프트보팅 방식이 다소 정확도가 높은 것으로 알려져 있다.  
실전적으로 활용되는 방식이므로 매우 중요하다. 
"""
```

### 회귀-VotingRegressor  

```python
from sklearn.linear_model import LinearRegression
```

### 분류-BaggingClassifier  

```python
from sklearn.svm import SVC  
from sklearn.ensemble import BaggingClassifier  

# 10개의 데이터셋에 SVC를 훈련시킨 10개 모델 결과를 종합한다.
model = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=0)

"""
종합정리  
배깅 방법은 다른 앙상블 방법에서도 나타나지만 다소 훈련데이터에 과대적합되는 경향을 보인다.  
그러나 이는 근본적인 문제는 아니다.  
기저모델의 개별 최적 하이퍼파라미터를 찾고 이를 배깅에 적재하면 더 좋은 결과를 얻을 수 있다.  
"""
```

### 앙상블 부스팅

```python
"""
부스팅은 여러 개의 약한 학습기를 순차적으로 학습시켜 예측하면서 잘 못 예측한 데이터에 가중치를 부여하여 오류를 개선해 나가며 학습하는 앙상블 모델이다. 위에 작성한 배깅이 한 번에 여러 개의 데이터셋에서 학습한 결과를 종합하는 병렬식 앙상블인 반면, 부스팅은 앞에서 학습한 모델 결과를 바탕으로 두 번째 모델에서 오류를 수정하고, 세 번째 모델에서도 오류에 중점을 두어 해결해 나가는 순차적인 직렬식 앙상블이라고 할 수 있다.  
"""

# 분류
# AdaBoosting 앙상블 모델 적용  
from sklearn.ensemble import AdaBoostClassifier  

# GradientBoosting 앙상블 모델 적용 
from sklearn.ensemble import GradientBoostingClassifier  


# 회귀  
# AdaBoosting 앙상블 모델 적용  
from sklearn.ensemble import AdaBoostRegressor  

# GradientBoostig 앙상블 모델 적용 
from sklearn.ensemble import GradientBoostingRegressor  


```



## metrics  

### **confusion_matrix**()

```python
# 혼동행렬 라이브러리 임포트  
from sklearn.metrics import confusion_matrix  

# 실제 y_train 데이터와, 모델이 예측한 pred_train을 비교
confusion_train = confusion_matrix(y_train, pred_train)
print("훈련데이터 오차행렬 : \n", confusion_train)
```

### **classification_report()**

```python
from sklearn.metrics import classification_report  

cfreport_train = classification_report(y_train, pred_train)

# 정확도 외에 정밀도(precision), 재현율(recall), f1-score 등이 제시된다.
print("분류예측 레포트 : \n", cfreport_train)
```

