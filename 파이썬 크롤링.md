크롤링 : 웹 페이지에서 필요한 데이터를 추출해내는 작업  
크롤러 : 크롤링을 하는 프로그램  

requests 라이브러리 : 웹 페이지의 HTML을 얻기 위해 사용하는 프로그램
BeautifulSoup 라이브러리 : 가져온 HTML, XML, JSON 등을 분석하기 위해 사용  


## BeautifulSoup  
```  
soup.find("p") # 처음 등장하는 태그 찾기
soup.find("div", class_="elice") # 특정 클래스 가진 태그 추출
soup.find("div", id ="elice")
soup.find_all("p") # 모든 태그 찾기 (리스트 반환)  
```  

.get_text() : 태그가 가지고 있는 텍스트를 얻을 수 있음 (태그 없이 추출)  

## requests  
: 파이썬에서 HTTP요청을 보낼 수 있는 모듈
GET 요청 ; 정보를 조회하기 위한 요청
POST 요청 : 정보를 생성, 변경하기 위한 요청  

```   
result = requests.get(url # 응답 받아 result에 저장
print(result.status_code) # 요청의 결과를 알 수 있음 
print(result.text) # 해당 웹 사이트의 HTML을 얻을 수 있음
```  

## 1. 하나의 페이지 크롤링  

네이버 헤드 뉴스 찾기  

```  
import requests
from bs4 import BeautifulSoup


def crawling(soup) :
    # soup 객체에서 추출해야 하는 정보를 찾고 반환하세요.
    result = []
    div = soup.find("div", class_="list_issue")
    for a in div.find_all("a"):
        result.append(a.get_text())
    return result
    

def main() :
    custom_header = {
        'referer' : 'https://www.naver.com/',
        'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'
    }
    
    url = "http://www.naver.com"
    req = requests.get(url, headers = custom_header)
    soup = BeautifulSoup(req.text, "html.parser")

    # crawling 함수의 결과를 출력합니다.
    print(crawling(soup))


if __name__ == "__main__" :
    main()

```  

영화 리뷰만 리스트로 모으기  
```  
import requests
from bs4 import BeautifulSoup

def crawling(soup) :
    # soup 객체에서 추출해야 하는 정보를 찾고 반환하세요.
    result = []
    tag_a = soup.find("ul", class_ = "rvw_list_area").find_all("a")
    for a in tag_a:
        if a.find("strong") == None:
            continue
        else:
            result.append(a.find("strong").get_text("strong"))
    return result
    


def main() :
    custom_header = {
        'referer' : 'https://movie.naver.com/',
        'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'
    }
    url = "https://movie.naver.com/movie/bi/mi/review.nhn?code=168058#"
    req = requests.get(url, headers = custom_header)
    soup = BeautifulSoup(req.text, "html.parser")

    # crawling 함수의 결과를 출력합니다.
    print(crawling(soup))


if __name__ == "__main__" :
    main()
```  

## 2. 여러 웹사이트 크롤링  
  
Query : 웹 서버에 GET 요청을 보낼 때 조건에 맞는 정보를 표현하기 위한 변수  
Ex) **번호**가 1번인 학생을 보여줘라.  

**children과 name**  
```  
soup.find("div").children  
# 특정 태그 (div)에 포함된 태그들의 리스트를 얻는 코드  
for child in children:
    print(child.name)  
# 어떤 태그의 이름을 알고 싶을 경우 name 속성을 이용  
```  

```  

import requests
from bs4 import BeautifulSoup

def crawling(soup) :
    # soup 객체에서 추출해야 하는 정보를 찾고 반환하세요.
    result = []
    ul = soup.find('ul', class_ = "list_news")
    for span in ul.find_all('span', class_ = 'tit'):
        result.append(span.get_text())
    return result
    


def main() :
    answer = []
    url = "https://sports.donga.com/ent"
    
    for i in range(0, 5):
        req = requests.get(url, params = {'p' : i*20 + 1}) # 쿼리와 반복분 이용, 일정 주기로 늘어나게 함
        soup = BeautifulSoup(req.text, "html.parser")
        
        answer += crawling(soup)

    # crawling 함수의 결과를 출력합니다.
    print(answer)


if __name__ == "__main__" :
    main()

```  



