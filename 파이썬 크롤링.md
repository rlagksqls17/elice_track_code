크롤링 : 웹 페이지에서 필요한 데이터를 추출해내는 작업  
크롤러 : 크롤링을 하는 프로그램  

requests 라이브러리 : 웹 페이지의 HTML을 얻기 위해 사용하는 프로그램
BeautifulSoup 라이브러리 : 가져온 HTML, XML, JSON 등을 분석하기 위해 사용  


## BeautifulSoup  
```  
soup.find("p") # 처음 등장하는 태그 찾기
soup.find("div", class_="elice") # 특정 클래스 가진 태그 추출
soup.find("div", id ="elice")
soup.find_all("p") # 모든 태그 찾기 (리스트 반환)  
```  

.get_text() : 태그가 가지고 있는 텍스트를 얻을 수 있음 (태그 없이 추출)  

## requests  
: 파이썬에서 HTTP요청을 보낼 수 있는 모듈
GET 요청 ; 정보를 조회하기 위한 요청
POST 요청 : 정보를 생성, 변경하기 위한 요청  

```   
result = requests.get(url # 응답 받아 result에 저장
print(result.status_code) # 요청의 결과를 알 수 있음 
print(result.text) # 해당 웹 사이트의 HTML을 얻을 수 있음
```  

## 실전 크롤링  

네이버 헤드 뉴스 찾기  

```  
import requests
from bs4 import BeautifulSoup


def crawling(soup) :
    # soup 객체에서 추출해야 하는 정보를 찾고 반환하세요.
    result = []
    div = soup.find("div", class_="list_issue")
    for a in div.find_all("a"):
        result.append(a.get_text())
    return result
    

def main() :
    custom_header = {
        'referer' : 'https://www.naver.com/',
        'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'
    }
    
    url = "http://www.naver.com"
    req = requests.get(url, headers = custom_header)
    soup = BeautifulSoup(req.text, "html.parser")

    # crawling 함수의 결과를 출력합니다.
    print(crawling(soup))


if __name__ == "__main__" :
    main()

```
